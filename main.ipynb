{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VrqMNaX7juMu"
   },
   "source": [
    "Import all libraries for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RCj3DtYQYWOj",
    "outputId": "4318e13f-a0c7-4f37-b559-438e0d20f755"
   },
   "outputs": [],
   "source": [
    "#Import libs\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve, average_precision_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, learning_curve\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import re\n",
    "import string\n",
    "\n",
    "#below imports are commented out as do not run in jupyter, some were used in google Colab \n",
    "# as they allowed us to generate useful graphs for the report\n",
    "#from lightgbm import LGBMClassifier\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.stem import PorterStemmer\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#from wordcloud import WordCloud\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rG-g0YGtJuJF"
   },
   "source": [
    "# Logistic Regression ( Dataset 1 : LingSpam )\n",
    "\n",
    "\n",
    "\n",
    "*   Hyperparameter\n",
    "*   TF-IDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1lidWfAI2A-"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('./messages.csv')\n",
    "\n",
    "# Replace NaN values with empty strings\n",
    "data.fillna(\"\", inplace=True)\n",
    "\n",
    "# Combine the 'subject' and 'message' columns\n",
    "data['combined_text'] = data['subject'] + ' ' + data['message']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['combined_text'], data['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jo4DPnE6I6pd"
   },
   "outputs": [],
   "source": [
    "# Apply TF-IDF with bigrams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "NiJyQ41kJBXb",
    "outputId": "53e4a9e7-1621-42f3-b125-185289db4c71"
   },
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning for Logistic Regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg_params = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "log_reg_grid = GridSearchCV(log_reg, log_reg_params, cv=5, n_jobs=-1)\n",
    "log_reg_grid.fit(X_train_tfidf, y_train)\n",
    "best_log_reg = log_reg_grid.best_estimator_\n",
    "\n",
    "# Train the best model on the training data\n",
    "best_log_reg.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Oi-mkWmJGvr"
   },
   "outputs": [],
   "source": [
    "# Test the model on the testing data\n",
    "y_pred_log_reg = best_log_reg.predict(X_test_tfidf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tCMuTTQXKyoq"
   },
   "source": [
    "Visualize the TF-IDF feature importances: You can visualize the top features (words or bigrams) with the highest TF-IDF scores to understand which features contribute the most to the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "MtrMwzO6JMZ5",
    "outputId": "d97f7036-7f17-4164-dfe6-6f53a1ff6693"
   },
   "outputs": [],
   "source": [
    "# Get the feature importances\n",
    "importances = best_log_reg.coef_[0]\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the indices sorted by importance\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "# Visualize the top k features\n",
    "k = 10\n",
    "top_k_features = [(feature_names[i], importances[i]) for i in indices[-k:]]\n",
    "top_k_features.reverse()\n",
    "\n",
    "# Plot the top k features\n",
    "plt.barh([x[0] for x in top_k_features], [x[1] for x in top_k_features])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top k Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_vHvrhh9Lmvy"
   },
   "source": [
    "Confusion Matrix: Visualize the confusion matrix to observe the classification performance and understand the false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "Z8x8r_KAK0ke",
    "outputId": "de32a05f-9e7b-4b16-a487-55c09949dca6"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_log_reg)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YMMNJnt6LsgJ"
   },
   "source": [
    "ROC Curve and AUC: Plot the Receiver Operating Characteristic (ROC) curve and compute the Area Under the Curve (AUC) to evaluate the model's ability to distinguish between spam and ham emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "PJxFro0GLVp8",
    "outputId": "847f1478-2f57-4c55-8fb5-380751a25771"
   },
   "outputs": [],
   "source": [
    "# Compute ROC curve and AUC\n",
    "y_pred_prob_log_reg = best_log_reg.predict_proba(X_test_tfidf)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_log_reg)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='AUC = %0.2f' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HRGX_jKcM0sL"
   },
   "source": [
    "The Precision-Recall curve shows the trade-off between precision and recall for different threshold values. This curve is useful when there is an imbalance in the distribution of classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "t45oLKwUMKoA",
    "outputId": "7440c546-6b62-4cd9-e31e-15ca75726a71"
   },
   "outputs": [],
   "source": [
    "precision, recall, _ = precision_recall_curve(y_test, best_log_reg.predict_proba(X_test_tfidf)[:, 1])\n",
    "average_precision = average_precision_score(y_test, y_pred_log_reg)\n",
    "\n",
    "plt.plot(recall, precision, label=f'Avg Precision: {average_precision:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bCveIKAENITw"
   },
   "source": [
    "A word cloud is a visual representation of the importance of words in a corpus, where the size of each word indicates its frequency or importance. Word clouds can help identify patterns and common words in spam and ham emails.\n",
    "\n",
    "N.B this is left over from running in google Colab, it doesn't run on jupyter however is left in for visual consistency. ( an image copy of running in colab is available in zipped file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 871
    },
    "id": "cyTJYtvlM6te",
    "outputId": "4bf1c696-35ac-4799-ccfd-9d35c442e257"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Separate ham and spam emails\n",
    "ham_emails = data[data['label'] == 0]['combined_text'].values\n",
    "spam_emails = data[data['label'] == 1]['combined_text'].values\n",
    "\n",
    "#ham_wordcloud = WordCloud(background_color='white', width=800, height=400).generate(\" \".join(ham_emails))\n",
    "#spam_wordcloud = WordCloud(background_color='white', width=800, height=400).generate(\" \".join(spam_emails))\n",
    "\n",
    "#plt.figure(figsize=(10, 5))\n",
    "#plt.imshow(ham_wordcloud, interpolation='bilinear')\n",
    "#plt.axis('off')\n",
    "#plt.title('Word Cloud for Ham Emails')\n",
    "#plt.show()\n",
    "\n",
    "#plt.figure(figsize=(10, 5))\n",
    "#plt.imshow(spam_wordcloud, interpolation='bilinear')\n",
    "#plt.axis('off')\n",
    "#plt.title('Word Cloud for Spam Emails')\n",
    "#plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "38t0KQ_kNYpt"
   },
   "source": [
    "Histogram of Email Lengths:\n",
    "Plotting histograms of email lengths can give insights into whether the length of an email can be a useful feature for classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "0CIk-ROZNTqG",
    "outputId": "772e1355-26f0-4e1d-f326-447b6af13848"
   },
   "outputs": [],
   "source": [
    "data['email_length'] = data['combined_text'].apply(lambda x: len(x))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(data[data['label'] == 0]['email_length'], bins=50, alpha=0.5, label='Ham')\n",
    "plt.hist(data[data['label'] == 1]['email_length'], bins=50, alpha=0.5, label='Spam')\n",
    "plt.xlabel('Email Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Email Lengths')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YItujzzvNf19"
   },
   "source": [
    "Boxplot of Email Lengths:\n",
    "Boxplots can be used to visualize the distribution of email lengths for both spam and ham emails, and identify possible outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "Fi3wDDOZNbkl",
    "outputId": "cf73016d-7be4-4ab2-975b-59b0d96f08f6"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 8))\n",
    "sns.boxplot(x='label', y='email_length', data=data, showfliers=False)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Email Length')\n",
    "plt.title('Boxplot of Email Lengths')\n",
    "plt.xticks([0, 1], ['Ham', 'Spam'])\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_Fn66i2zOJMO"
   },
   "source": [
    "Bar Chart of Top N-grams:\n",
    "A bar chart can be used to visualize the most frequent n-grams in spam and ham emails. This can provide insights into which n-grams are more prevalent in spam or ham emails and can be useful for understanding the types of words and phrases that characterize each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 991
    },
    "id": "oVrGINsPOB3x",
    "outputId": "792cfb13-7967-446d-bedf-bf67be5e0608"
   },
   "outputs": [],
   "source": [
    "def plot_top_ngrams(corpus, ngram_range, top_n, title):\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    X_count = count_vectorizer.fit_transform(corpus)\n",
    "    ngrams = count_vectorizer.get_feature_names_out()\n",
    "    ngram_counts = X_count.sum(axis=0).A1\n",
    "    sorted_ngrams = sorted(zip(ngrams, ngram_counts), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(*zip(*sorted_ngrams))\n",
    "    plt.xlabel('N-grams')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "plot_top_ngrams(spam_emails, (1, 1), 10, 'Top 10 Unigrams in Spam Emails')\n",
    "plot_top_ngrams(ham_emails, (1, 1), 10, 'Top 10 Unigrams in Ham Emails')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wTTt2X_HRpJB"
   },
   "source": [
    "A learning curve is a plot that shows the relationship between the number of training samples and the model's performance. It can help to identify if the model is overfitting, underfitting, or well-fitted to the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "X08nssZyOkRJ",
    "outputId": "6c4adcc0-ace1-49a8-a9bc-6e33029e2446"
   },
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(best_log_reg, X_train_tfidf, y_train, cv=5)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_scores_mean, label='Training score')\n",
    "plt.plot(train_sizes, test_scores_mean, label='Cross-validation score')\n",
    "plt.xlabel('Training samples')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j1_DJmf7musl",
    "outputId": "cd0be24b-3b85-44ff-94f7-b108fff276eb"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "precision_log_reg = precision_score(y_test, y_pred_log_reg)\n",
    "recall_log_reg = recall_score(y_test, y_pred_log_reg)\n",
    "f1_log_reg = f1_score(y_test, y_pred_log_reg)\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"Accuracy:\", accuracy_log_reg)\n",
    "print(\"Precision:\", precision_log_reg)\n",
    "print(\"Recall:\", recall_log_reg)\n",
    "print(\"F1 Score:\", f1_log_reg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3m4kpFMW2CsS"
   },
   "source": [
    "## Before applying TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQ3P9-hQu22U",
    "outputId": "28cadafb-4e5c-4c08-ed66-15cbee72cb81"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('./messages.csv')\n",
    "\n",
    "# Replace NaN values with empty strings\n",
    "data.fillna(\"\", inplace=True)\n",
    "\n",
    "# Combine the 'subject' and 'message' columns\n",
    "data['combined_text'] = data['subject'] + ' ' + data['message']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['combined_text'], data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Count the number of words in each message\n",
    "X_train_counts = X_train.apply(lambda x: len(x.split()))\n",
    "X_test_counts = X_test.apply(lambda x: len(x.split()))\n",
    "\n",
    "# Create a Logistic Regression model with hyperparameter tuning\n",
    "log_reg = LogisticRegression()\n",
    "log_reg_params = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "log_reg_grid = GridSearchCV(log_reg, log_reg_params, cv=5, n_jobs=-1)\n",
    "log_reg_grid.fit(X_train_counts.values.reshape(-1, 1), y_train)\n",
    "\n",
    "# Train the best Logistic Regression model found during the grid search\n",
    "best_log_reg = log_reg_grid.best_estimator_\n",
    "\n",
    "# Test the model on the testing data\n",
    "y_pred_log_reg = best_log_reg.predict(X_test_counts.values.reshape(-1, 1))\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "precision_log_reg = precision_score(y_test, y_pred_log_reg)\n",
    "recall_log_reg = recall_score(y_test, y_pred_log_reg)\n",
    "f1_log_reg = f1_score(y_test, y_pred_log_reg)\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"Accuracy:\", accuracy_log_reg)\n",
    "print(\"Precision:\", precision_log_reg)\n",
    "print(\"Recall:\", recall_log_reg)\n",
    "print(\"F1 Score:\", f1_log_reg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wBA_dL7e2hSw"
   },
   "source": [
    "Before using TF-IDF for preprocessing, we achieved an accuracy of 0.80. However, after implementing TF-IDF, the accuracy improved significantly to 0.99. This suggests that using TF-IDF as a preprocessing step helped to identify and weigh the important words in the text, leading to better classification results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zMTXiWe40pf2"
   },
   "source": [
    "# Logistic Regression ( Dataset 2 : SpamAssassin )\n",
    "\n",
    "\n",
    "\n",
    "*   Hyperparameter\n",
    "*   TF-IDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_I-sFXGvSjX",
    "outputId": "fb5692a6-23de-4f80-a140-70753ae90a24"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('./completeSpamAssassin.csv')\n",
    "\n",
    "# Replace NaN values with empty strings\n",
    "data.fillna(\"\", inplace=True)\n",
    "\n",
    "# Combine the 'subject' and 'message' columns\n",
    "data['combined_text'] = data['Unnamed: 0'].astype(str) + ' ' + data['Body']\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', analyzer='word', tokenizer=None, preprocessor=None, \n",
    "                             max_features=None, lowercase=True, strip_accents=None, binary=False, \n",
    "                             ngram_range=(1, 1), max_df=1.0, min_df=1)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "data['combined_text'] = data['combined_text'].apply(preprocess_text)\n",
    "data_counts = vectorizer.fit_transform(data['combined_text'])\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['combined_text'], data['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply TF-IDF with bigrams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Perform hyperparameter tuning for Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=5000)\n",
    "log_reg_params = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "log_reg_grid = GridSearchCV(log_reg, log_reg_params, cv=5, n_jobs=-1)\n",
    "log_reg_grid.fit(X_train_tfidf, y_train)\n",
    "best_log_reg = log_reg_grid.best_estimator_\n",
    "\n",
    "# Train the best model on the training data\n",
    "best_log_reg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Test the model on the testing data\n",
    "y_pred_log_reg = best_log_reg.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "precision_log_reg = precision_score(y_test, y_pred_log_reg)\n",
    "recall_log_reg = recall_score(y_test, y_pred_log_reg)\n",
    "f1_log_reg = f1_score(y_test, y_pred_log_reg)\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"Accuracy:\", accuracy_log_reg)\n",
    "print(\"Precision:\", precision_log_reg)\n",
    "print(\"Recall:\", recall_log_reg)\n",
    "print(\"F1 Score:\", f1_log_reg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QbBStfi623An"
   },
   "source": [
    "The high accuracy of 0.96 achieved on the spamassassin dataset further demonstrates the effectiveness of the model incorporating TF-IDF as a preprocessing step, indicating that it performs well not only on the initial dataset but also on other similar datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "y-hh93yIEnLW"
   },
   "source": [
    "# Alternative Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HDFe91hsEsbf",
    "outputId": "fafd51da-d523-4f9f-ef42-d1e1a2df3b20"
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "data=pd.read_csv('./messages.csv')\n",
    "\n",
    "# Preprocessing\n",
    "data.fillna(\"\", inplace=True)\n",
    "data['combined_text'] = data['subject'] + ' ' + data['message']\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['combined_text'], data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature extraction\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Model training and evaluation\n",
    "log_reg = LogisticRegression()\n",
    "log_reg_params = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "log_reg_grid = GridSearchCV(log_reg, log_reg_params, cv=5, n_jobs=-1)\n",
    "log_reg_grid.fit(X_train_tfidf, y_train)\n",
    "best_log_reg = log_reg_grid.best_estimator_\n",
    "\n",
    "best_log_reg.fit(X_train_tfidf, y_train)\n",
    "y_pred_log_reg = best_log_reg.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "precision_log_reg = precision_score(y_test, y_pred_log_reg)\n",
    "recall_log_reg = recall_score(y_test, y_pred_log_reg)\n",
    "f1_log_reg = f1_score(y_test, y_pred_log_reg)\n",
    "\n",
    "print(\"Dataset 1 - Logistic Regression:\")\n",
    "print(\"Accuracy:\", accuracy_log_reg)\n",
    "print(\"Precision:\", precision_log_reg)\n",
    "print(\"Recall:\", recall_log_reg)\n",
    "print(\"F1 Score:\", f1_log_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 976
    },
    "id": "3wEkrYv_FY0_",
    "outputId": "006d97e5-0104-40c7-a6c1-6da1a492f2bf"
   },
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train_tfidf, y_train)\n",
    "y_pred_naive_bayes = naive_bayes.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate scores\n",
    "accuracy_naive_bayes = accuracy_score(y_test, y_pred_naive_bayes)\n",
    "precision_naive_bayes = precision_score(y_test, y_pred_naive_bayes)\n",
    "recall_naive_bayes = recall_score(y_test, y_pred_naive_bayes)\n",
    "f1_naive_bayes = f1_score(y_test, y_pred_naive_bayes)\n",
    "\n",
    "# Print scores\n",
    "print(\"Naive Bayes - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}\".format(accuracy_naive_bayes, precision_naive_bayes, recall_naive_bayes, f1_naive_bayes))\n",
    "\n",
    "# Support Vector Machines (SVM)\n",
    "svm = SVC(kernel='linear', probability=True)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred_svm = svm.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate scores\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "precision_svm = precision_score(y_test, y_pred_svm)\n",
    "recall_svm = recall_score(y_test, y_pred_svm)\n",
    "f1_svm = f1_score(y_test, y_pred_svm)\n",
    "\n",
    "# Print scores\n",
    "print(\"SVM - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}\".format(accuracy_svm, precision_svm, recall_svm, f1_svm))\n",
    "\n",
    "\n",
    "# Comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Model\": [\"Logistic Regression\", \"Naive Bayes\", \"Support Vector Machines\"],\n",
    "    \"Accuracy\": [accuracy_log_reg, accuracy_naive_bayes, accuracy_svm],\n",
    "    \"Precision\": [precision_log_reg, precision_naive_bayes, precision_svm],\n",
    "    \"Recall\": [recall_log_reg, recall_naive_bayes, recall_svm],\n",
    "    \"F1 Score\": [f1_log_reg, f1_naive_bayes, f1_svm]\n",
    "})\n",
    "\n",
    "# Print comparison dataframe\n",
    "print(comparison_df)\n",
    "\n",
    "# Plot comparison dataframe\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "comparison_df.plot(kind=\"bar\", ax=ax)\n",
    "ax.set_xticks(comparison_df.index)\n",
    "ax.set_xticklabels(comparison_df[\"Model\"], rotation=45)\n",
    "ax.set_title(\"Model Comparison on LingSpam data set\")\n",
    "ax.set_xlabel(\"Models\")\n",
    "ax.set_ylabel(\"Scores\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AAL5HRxO3u22"
   },
   "source": [
    "Based on the comparison of different models on the LingSpam dataset, Logistic Regression and SVM show the same accuracy, precision, recall, and F1 score, with Logistic Regression having a faster runtime. Moreover, Logistic Regression outperforms all other models, including Naive Bayes and LightGBM, in terms of accuracy, precision, recall, and F1 score, indicating its superiority and suitability for spam detection across different datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# below code is included for continuity - will not work in jupyterhub\n",
    "The code below is the comparison using an LGBM ensemble method that is unable to be imported into jupyterhub. Code is retained for consistency and can run in other vscode/google colab providing the necessary imports are uncommented as it was used for graph generation in our report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train_tfidf, y_train)\n",
    "y_pred_naive_bayes = naive_bayes.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate scores\n",
    "accuracy_naive_bayes = accuracy_score(y_test, y_pred_naive_bayes)\n",
    "precision_naive_bayes = precision_score(y_test, y_pred_naive_bayes)\n",
    "recall_naive_bayes = recall_score(y_test, y_pred_naive_bayes)\n",
    "f1_naive_bayes = f1_score(y_test, y_pred_naive_bayes)\n",
    "\n",
    "# Print scores\n",
    "print(\"Naive Bayes - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}\".format(accuracy_naive_bayes, precision_naive_bayes, recall_naive_bayes, f1_naive_bayes))\n",
    "\n",
    "# Support Vector Machines (SVM)\n",
    "svm = SVC(kernel='linear', probability=True)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred_svm = svm.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate scores\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "precision_svm = precision_score(y_test, y_pred_svm)\n",
    "recall_svm = recall_score(y_test, y_pred_svm)\n",
    "f1_svm = f1_score(y_test, y_pred_svm)\n",
    "\n",
    "# Print scores\n",
    "print(\"SVM - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}\".format(accuracy_svm, precision_svm, recall_svm, f1_svm))\n",
    "\n",
    "\n",
    "# Ensemble method - LightGBM\n",
    "lgbm = LGBMClassifier()\n",
    "lgbm.fit(X_train_tfidf, y_train)\n",
    "y_pred_lgbm = lgbm.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate scores\n",
    "accuracy_lgbm = accuracy_score(y_test, y_pred_lgbm)\n",
    "precision_lgbm = precision_score(y_test, y_pred_lgbm)\n",
    "recall_lgbm = recall_score(y_test, y_pred_lgbm)\n",
    "f1_lgbm = f1_score(y_test, y_pred_lgbm)\n",
    "\n",
    "# Print scores\n",
    "print(\"LightGBM - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}\".format(accuracy_lgbm, precision_lgbm, recall_lgbm, f1_lgbm))\n",
    "\n",
    "# Comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Model\": [\"Logistic Regression\", \"Naive Bayes\", \"Support Vector Machines\", \"LightGBM\"],\n",
    "    \"Accuracy\": [accuracy_log_reg, accuracy_naive_bayes, accuracy_svm, accuracy_lgbm],\n",
    "    \"Precision\": [precision_log_reg, precision_naive_bayes, precision_svm, precision_lgbm],\n",
    "    \"Recall\": [recall_log_reg, recall_naive_bayes, recall_svm, recall_lgbm],\n",
    "    \"F1 Score\": [f1_log_reg, f1_naive_bayes, f1_svm, f1_lgbm]\n",
    "})\n",
    "\n",
    "# Print comparison dataframe\n",
    "print(comparison_df)\n",
    "\n",
    "# Plot comparison dataframe\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "comparison_df.plot(kind=\"bar\", ax=ax)\n",
    "ax.set_xticks(comparison_df.index)\n",
    "ax.set_xticklabels(comparison_df[\"Model\"], rotation=45)\n",
    "ax.set_title(\"Model Comparison on LingSpam data set\")\n",
    "ax.set_xlabel(\"Models\")\n",
    "ax.set_ylabel(\"Scores\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Machine Learning",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
